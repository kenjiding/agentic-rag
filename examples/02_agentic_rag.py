"""Agentic RAG åŸºç¡€ç¤ºä¾‹"""
import sys
from pathlib import Path
import os

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils import get_absolute_path
from src.agentic_rag.docs_parser import DocsParser
from dotenv import load_dotenv
from langchain_core.documents import Document
from src.agentic_rag.agentic_rag import AgenticRAG
from src.agentic_rag.parser import PDFParser

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("Agentic RAG ç³»ç»Ÿç¤ºä¾‹")
    print("=" * 60)
    print("\næœ¬ç¤ºä¾‹å±•ç¤º Agentic RAG å¦‚ä½•é€šè¿‡è¿­ä»£ä¼˜åŒ–æ”¹è¿›æ£€ç´¢å’Œç”Ÿæˆè´¨é‡ã€‚")
    
    # 1. åˆå§‹åŒ– Agentic RAG ç³»ç»Ÿ
    print("\n[æ­¥éª¤ 1] åˆå§‹åŒ– Agentic RAG ç³»ç»Ÿ...")
    rag = AgenticRAG(
        model_name="gpt-4o-mini",
        max_iterations=5,
        persist_directory="./tmp/chroma_db/agentic_rag"
    )

    current_file = os.path.dirname(__file__)
    # md_file = get_absolute_path(current_file, "../README.md")
    # docs_parser = DocsParser(file_path=md_file)

    # pdf_path = get_absolute_path(current_file, "../kenjiding.pdf")
    # pdf_parser = DocsParser(file_path=pdf_path)
    
    # # 2. å‡†å¤‡æ–‡æ¡£
    # print("\n[æ­¥éª¤ 2] åŠ è½½æ–‡æ¡£...")
    # sample_docs = [
    #     *pdf_parser.docs,
    #     *docs_parser.docs,
    #     Document(page_content="""
    #     LangGraph æ˜¯ LangChain çš„ä¸€ä¸ªæ‰©å±•åº“ï¼Œä¸“é—¨ç”¨äºæ„å»ºæœ‰çŠ¶æ€çš„ã€å¤šå‚ä¸è€…çš„åº”ç”¨ç¨‹åºã€‚
        
    #     æ ¸å¿ƒç‰¹æ€§ï¼š
    #     1. åŸºäºå›¾çš„æ‰§è¡Œæ¨¡å‹ï¼šä½¿ç”¨èŠ‚ç‚¹å’Œè¾¹æ¥å®šä¹‰å·¥ä½œæµ
    #     2. çŠ¶æ€ç®¡ç†ï¼šè‡ªåŠ¨ç®¡ç†å’Œä¼ é€’çŠ¶æ€
    #     3. å¾ªç¯æ”¯æŒï¼šæ”¯æŒæ¡ä»¶å¾ªç¯å’Œè¿­ä»£
    #     4. æ£€æŸ¥ç‚¹ï¼šå¯ä»¥ä¿å­˜å’Œæ¢å¤æ‰§è¡ŒçŠ¶æ€
        
    #     LangGraph ç‰¹åˆ«é€‚åˆæ„å»ºï¼š
    #     - èŠå¤©æœºå™¨äºº
    #     - å¤šæ­¥éª¤æ¨ç†ç³»ç»Ÿ
    #     - Agentic RAG ç³»ç»Ÿ
    #     - å¤æ‚çš„å·¥ä½œæµåº”ç”¨
    #     """),
    #     Document(page_content="""
    #     ä¼ ç»Ÿ RAG ç³»ç»Ÿä½¿ç”¨çº¿æ€§æµç¨‹ï¼šç”¨æˆ·é—®é¢˜ â†’ å‘é‡åŒ– â†’ æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£ â†’ æ‹¼æ¥ä¸Šä¸‹æ–‡ â†’ LLM ç”Ÿæˆå›ç­”ã€‚
        
    #     ä¼ ç»Ÿ RAG çš„å±€é™æ€§ï¼š
    #     1. æ£€ç´¢è´¨é‡ä¾èµ–æŸ¥è¯¢è´¨é‡ï¼šå¦‚æœåˆå§‹æŸ¥è¯¢ä¸å¤Ÿå¥½ï¼Œæ£€ç´¢ç»“æœå°±ä¼šå¾ˆå·®
    #     2. æ— æ³•å¤„ç†å¤æ‚æŸ¥è¯¢ï¼šæ— æ³•åˆ†è§£å¤šæ­¥éª¤é—®é¢˜
    #     3. ç¼ºä¹åé¦ˆæœºåˆ¶ï¼šæ£€ç´¢å¤±è´¥æ—¶æ— æ³•è‡ªæˆ‘è°ƒæ•´
    #     4. ä¸Šä¸‹æ–‡åˆ©ç”¨æœ‰é™ï¼šæ— æ³•æ ¹æ®ç”Ÿæˆçš„ä¸­é—´ç»“æœè¿›è¡ŒåŠ¨æ€æ£€ç´¢
    #     """),
    #     Document(page_content="""
    #     Agentic RAG å°† Agentï¼ˆæ™ºèƒ½ä½“ï¼‰çš„æ€æƒ³å¼•å…¥ RAG ç³»ç»Ÿï¼Œä½¿å…¶èƒ½å¤Ÿï¼š
    #     1. ä¸»åŠ¨å†³ç­–ï¼šæ ¹æ®å½“å‰çŠ¶æ€å†³å®šä¸‹ä¸€æ­¥è¡ŒåŠ¨
    #     2. è¿­ä»£ä¼˜åŒ–ï¼šå¯ä»¥å¤šè½®æ£€ç´¢å’Œç”Ÿæˆ
    #     3. å·¥å…·è°ƒç”¨ï¼šå¯ä»¥ä½¿ç”¨å¤šç§å·¥å…·ï¼ˆæ£€ç´¢å™¨ã€è®¡ç®—å™¨ã€ä»£ç æ‰§è¡Œå™¨ç­‰ï¼‰
    #     4. è‡ªæˆ‘åæ€ï¼šèƒ½å¤Ÿè¯„ä¼°ç»“æœè´¨é‡å¹¶è¿›è¡Œæ”¹è¿›
        
    #     Agentic RAG çš„æ ¸å¿ƒç»„ä»¶ï¼š
    #     - å†³ç­–å¼•æ“ï¼šå†³å®šä¸‹ä¸€æ­¥åº”è¯¥åšä»€ä¹ˆ
    #     - æ£€ç´¢å™¨ï¼šå¤šç§æ£€ç´¢ç­–ç•¥ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡è°ƒæ•´
    #     - ç”Ÿæˆå™¨ï¼šåŸºäºæ£€ç´¢ç»“æœç”Ÿæˆå›ç­”ï¼Œå¯æ”¹è¿›
    #     - è¯„ä¼°å™¨ï¼šè¯„ä¼°æ£€ç´¢å’Œç”Ÿæˆçš„è´¨é‡
    #     - çŠ¶æ€ç®¡ç†å™¨ï¼šç»´æŠ¤å¯¹è¯å†å²å’Œä¸­é—´ç»“æœ
    #     """)
    # ]
    
    # pdf_path = get_absolute_path(current_file, "../uber_10q_march_2022_page26.pdf")
    # pdf_parser = PDFParser()
    # pdf_chunks = pdf_parser.parse_pdf_to_documents(pdf_path, refresh=True)
    # pdf_parser = DocsParser(file_path=pdf_path)
    # # 3. æ„å»ºå‘é‡æ•°æ®åº“
    # print("\n[æ­¥éª¤ 3] æ„å»ºå‘é‡æ•°æ®åº“...")
    # # rag.build_vectorstore(sample_docs)
    # rag.add_documents(pdf_chunks)
    
    # 4. æµ‹è¯•æŸ¥è¯¢ - å±•ç¤ºè¿­ä»£ä¼˜åŒ–
    print("\n[æ­¥éª¤ 4] æµ‹è¯•æŸ¥è¯¢ï¼ˆå±•ç¤º Agentic RAG çš„è¿­ä»£ä¼˜åŒ–èƒ½åŠ›ï¼‰...")
    questions = [
        "Uber 2021å¹´å’Œ2022å¹´Legal, tax, and regulatory reserve changes and settlements ä¸šåŠ¡çš„è°ƒæ•´åEBITDAåˆ†åˆ«æ˜¯å¤šå°‘?",
        # "2022å¹´ç¦å¸ƒæ–¯å¯Œè±ªæ¦œæ°å¤«Â·è´ç´¢æ–¯è´¢å¯Œæ˜¯å¤šå°‘?",
        # "2019å¹´ç¦å¸ƒæ–¯å¯Œè±ªæ¦œæ°å¤«Â·è´ç´¢æ–¯è´¢å¯Œæ˜¯å¤šå°‘?",
        # "2019å¹´, 2020,2021å¹´ç¦å¸ƒæ–¯å¯Œè±ªæ¦œæ°å¤«Â·è´ç´¢æ–¯è´¢å¯Œæ˜¯ä¸Šå‡äº†è¿˜æ˜¯ä¸‹é™äº†? è¯·ç»™å‡ºå…·ä½“æ•°æ®.",
        # "kenjidingçš„low codeé¡¹ç›®æ˜¯åœ¨å“ªå®¶å…¬å¸åšçš„?",
        # "kenjidingæœ‰å“ªäº›å…¬å¸å·¥ä½œè¿‡?",
        # "æ®ä½ çš„äº†è§£,kenjidingæœ€å‰å®³çš„ç»å†æ˜¯å“ªäº›?",
        # "gentic-agenté¡¹ç›®ç»“æ„æ˜¯æ€æ ·çš„?",
        # "LangGraph çš„æ ¸å¿ƒç‰¹æ€§æ˜¯ä»€ä¹ˆï¼Ÿ",
        # "ä¼ ç»Ÿ RAG æœ‰ä»€ä¹ˆå±€é™æ€§ï¼Ÿ",
        # "Agentic RAG å¦‚ä½•æ”¹è¿›ä¼ ç»Ÿ RAGï¼Ÿ"
    ]
    
    for i, question in enumerate(questions, 1):
        print(f"\n{'='*60}")
        print(f"æµ‹è¯• {i}/{len(questions)}: {question}")
        print(f"{'='*60}\n")
        
        result = rag.query(question, verbose=True)
        
        print(f"\nğŸ“Š æ‰§è¡Œç»Ÿè®¡:")
        print(f"  æ€»è¿­ä»£æ¬¡æ•°: {result.get('iteration_count', 0)}")
        print(f"  æ£€ç´¢è½®æ•°: {len(result.get('retrieval_history', []))}")
        print(f"  æœ€ç»ˆæ£€ç´¢è´¨é‡: {result.get('retrieval_quality', 0.0):.2f}")
        print(f"  æœ€ç»ˆç­”æ¡ˆè´¨é‡: {result.get('answer_quality', 0.0):.2f}")
        
        print(f"\nğŸ’¡ æœ€ç»ˆç­”æ¡ˆ:")
        print(f"{result['answer']}\n")
        
        # å±•ç¤ºæ£€ç´¢å†å²
        if result.get("retrieval_history"):
            print("ğŸ“š æ£€ç´¢å†å²:")
            for j, docs in enumerate(result["retrieval_history"], 1):
                print(f"  ç¬¬ {j} è½®: æ£€ç´¢åˆ° {len(docs)} ä¸ªæ–‡æ¡£å—")
        
        print("-" * 60)
    
    print("\n" + "=" * 60)
    print("Agentic RAG ç¤ºä¾‹å®Œæˆï¼")
    print("\nğŸ’¡ è§‚å¯Ÿè¦ç‚¹:")
    print("1. Agentic RAG ä¼šæ ¹æ®è´¨é‡è‡ªåŠ¨å†³å®šæ˜¯å¦éœ€è¦æ›´å¤šè¿­ä»£")
    print("2. å¦‚æœæ£€ç´¢è´¨é‡ä¸å¤Ÿï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨å°è¯•ä¸åŒçš„æ£€ç´¢ç­–ç•¥")
    print("3. å¦‚æœç­”æ¡ˆè´¨é‡ä¸å¤Ÿï¼Œç³»ç»Ÿä¼šå°è¯•æ”¹è¿›æˆ–è·å–æ›´å¤šä¿¡æ¯")
    print("=" * 60)


if __name__ == "__main__":
    main()